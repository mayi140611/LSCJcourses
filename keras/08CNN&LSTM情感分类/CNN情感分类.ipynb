{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ian\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import jieba # pip install jieba\n",
    "\n",
    "from keras.layers import Dense, Input, Flatten, Dropout\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, concatenate\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读入数据\n",
    "neg=pd.read_excel('data/neg.xls',header=None)\n",
    "pos=pd.read_excel('data/pos.xls',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>做为一本声名在外的流行书，说的还是广州的外企，按道理应该和我的生存环境差不多啊。但是一看之下...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>作者有明显的自恋倾向，只有有老公养不上班的太太们才能像她那样生活。很多方法都不实用，还有抄袭...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>作者完全是以一个过来的自认为是成功者的角度去写这个问题，感觉很不客观。虽然不是很喜欢，但是，...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>作者提倡内调，不信任化妆品，这点赞同。但是所列举的方法太麻烦，配料也不好找。不是太实用。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>作者的文笔一般，观点也是和市面上的同类书大同小异，不推荐读者购买。</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0  做为一本声名在外的流行书，说的还是广州的外企，按道理应该和我的生存环境差不多啊。但是一看之下...\n",
       "1  作者有明显的自恋倾向，只有有老公养不上班的太太们才能像她那样生活。很多方法都不实用，还有抄袭...\n",
       "2  作者完全是以一个过来的自认为是成功者的角度去写这个问题，感觉很不客观。虽然不是很喜欢，但是，...\n",
       "3       作者提倡内调，不信任化妆品，这点赞同。但是所列举的方法太麻烦，配料也不好找。不是太实用。\n",
       "4                  作者的文笔一般，观点也是和市面上的同类书大同小异，不推荐读者购买。"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10428\n",
      "10677\n"
     ]
    }
   ],
   "source": [
    "#合并语料\n",
    "pn = pd.concat([pos,neg],ignore_index=True) \n",
    "#计算语料数目\n",
    "neglen = len(neg)\n",
    "print(neglen)\n",
    "poslen = len(pos) \n",
    "print(poslen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache C:\\Users\\Ian\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 1.058 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "#定义分词函数\n",
    "cw = lambda x: list(jieba.cut(x))\n",
    "pn['words'] = pn[0].apply(cw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>做父母一定要有刘墉这样的心态，不断地学习，不断地进步，不断地给自己补充新鲜血液，让自己保持一...</td>\n",
       "      <td>[做, 父母, 一定, 要, 有, 刘墉, 这样, 的, 心态, ，, 不断, 地, 学习,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>作者真有英国人严谨的风格，提出观点、进行论述论证，尽管本人对物理学了解不深，但是仍然能感受到...</td>\n",
       "      <td>[作者, 真有, 英国人, 严谨, 的, 风格, ，, 提出, 观点, 、, 进行, 论述,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>作者长篇大论借用详细报告数据处理工作和计算结果支持其新观点。为什么荷兰曾经县有欧洲最高的生产...</td>\n",
       "      <td>[作者, 长篇大论, 借用, 详细, 报告, 数据处理, 工作, 和, 计算结果, 支持, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>作者在战几时之前用了＂拥抱＂令人叫绝．日本如果没有战败，就有会有美军的占领，没胡官僚主义的延...</td>\n",
       "      <td>[作者, 在, 战, 几时, 之前, 用, 了, ＂, 拥抱, ＂, 令人, 叫绝, ．, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>作者在少年时即喜阅读，能看出他精读了无数经典，因而他有一个庞大的内心世界。他的作品最难能可贵...</td>\n",
       "      <td>[作者, 在, 少年, 时即, 喜, 阅读, ，, 能, 看出, 他, 精读, 了, 无数,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>作者有一种专业的谨慎，若能有幸学习原版也许会更好，简体版的书中的印刷错误比较多，影响学者理解...</td>\n",
       "      <td>[作者, 有, 一种, 专业, 的, 谨慎, ，, 若能, 有幸, 学习, 原版, 也许, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>作者用诗一样的语言把如水般清澈透明的思想娓娓道来，像一个经验丰富的智慧老人为我们解开一个又一...</td>\n",
       "      <td>[作者, 用, 诗, 一样, 的, 语言, 把, 如水般, 清澈, 透明, 的, 思想, 娓...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>作者提出了一种工作和生活的方式，作为咨询界的元老，不仅能提出理念，而且能够身体力行地实践，并...</td>\n",
       "      <td>[作者, 提出, 了, 一种, 工作, 和, 生活, 的, 方式, ，, 作为, 咨询, 界...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>作者妙语连珠，将整个60-70年代用层出不穷的摇滚巨星与自身故事紧紧相连什么是乡愁？什么是摇...</td>\n",
       "      <td>[作者, 妙语连珠, ，, 将, 整个, 60, -, 70, 年代, 用, 层出不穷, 的...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>作者逻辑严密，一气呵成。没有一句废话，深入浅出，循循善诱，环环相扣。让平日里看到指标图释就头...</td>\n",
       "      <td>[作者, 逻辑, 严密, ，, 一气呵成, 。, 没有, 一句, 废话, ，, 深入浅出, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>作者力从马克思注意经济学角度来剖析当代中国经济细心的人会发现中国近20年来的一些政策措施在何...</td>\n",
       "      <td>[作者, 力, 从, 马克思, 注意, 经济学, 角度, 来, 剖析, 当代, 中国, 经济...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>作者结合希尔和卡耐基、汪中求等大师的一些观点，结合中国的实际情况，给渴望成功的青年指出一条实...</td>\n",
       "      <td>[作者, 结合, 希尔, 和, 卡耐基, 、, 汪中求, 等, 大师, 的, 一些, 观点,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>作者更多的是从圆圆母亲的角度来写这个文章。90%的例子都是以圆圆为中心的。我认为，例证不够充...</td>\n",
       "      <td>[作者, 更, 多, 的, 是从, 圆圆, 母亲, 的, 角度, 来, 写, 这个, 文章,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>作者对于某些电影“表面粗糙”、“内里光滑”的分析，可谓入木三分，为理解那些貌似触及现实而又让...</td>\n",
       "      <td>[作者, 对于, 某些, 电影, “, 表面, 粗糙, ”, 、, “, 内里, 光滑, ”...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>作者的理念很好，主要是看一个公司的内部情况来决定股票的情况。操作比较难，作者也承认，所以他说...</td>\n",
       "      <td>[作者, 的, 理念, 很, 好, ，, 主要, 是, 看, 一个, 公司, 的, 内部, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>作者的观点独特，语言犀利，深刻的总结了男人与女人之间的是非恩怨，把男人与女人在生活中积累的宿...</td>\n",
       "      <td>[作者, 的, 观点, 独特, ，, 语言, 犀利, ，, 深刻, 的, 总结, 了, 男人...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>作者的笔触很真实 将一个不忠却又深爱自己妻子的丈夫形象毫无遮掩的展示给读者 而什么是真爱 什...</td>\n",
       "      <td>[作者, 的, 笔触, 很, 真实,  , 将, 一个, 不忠, 却, 又, 深爱, 自己,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>作者笔下留情啊，深圳的自由作家远远没有《离婚未遂》笔下那么潇洒。事实上，深圳是一个典型的伪文...</td>\n",
       "      <td>[作者, 笔下留情, 啊, ，, 深圳, 的, 自由, 作家, 远远, 没有, 《, 离婚,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>作者被认为是“爱的奇迹天使”，确实是这样的。很多人认为与孩子的相处一定充满快乐，但却不知一份...</td>\n",
       "      <td>[作者, 被, 认为, 是, “, 爱, 的, 奇迹, 天使, ”, ，, 确实, 是, 这...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>作为有史以来最伟大的基金经理彼得&amp;#183;林奇凭借其在投资领域杰出的贡献，终其一生的经验和...</td>\n",
       "      <td>[作为, 有史以来, 最, 伟大, 的, 基金, 经理, 彼得, &amp;#, 183, ;, 林...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>作为一名山西太谷人，从小听多了有关晋商的故事，也去过许多山西的地方，但由于离家已久，故乡渐渐...</td>\n",
       "      <td>[作为, 一名, 山西, 太谷, 人, ，, 从小, 听, 多, 了, 有关, 晋商, 的,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>作为一名“白骨精”，我不得不佩服作者入木三分的刻画，不仅精准地描绘出我们的生活画面，更深刻地...</td>\n",
       "      <td>[作为, 一名, “, 白骨精, ”, ，, 我, 不得不, 佩服, 作者, 入木三分, 的...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>作为一本职场启示录，不错没有像杜拉拉一样的引入胜的故事，却也用轻松的方式告诉你，如何在职场中...</td>\n",
       "      <td>[作为, 一本, 职场, 启示录, ，, 不错, 没有, 像, 杜, 拉拉, 一样, 的, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>作为一本屹立不倒的经典教材，纽摄所传授的不是单纯的技术，而是理念。不是教条的灌输，而是互动性...</td>\n",
       "      <td>[作为, 一本, 屹立, 不倒, 的, 经典, 教材, ，, 纽摄, 所, 传授, 的, 不...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>作为一本西方人创作的心理学读物，还是以西方人的视角来看待问题的方式来阐述，目前刚刚开始读，但...</td>\n",
       "      <td>[作为, 一本, 西方人, 创作, 的, 心理学, 读物, ，, 还是, 以, 西方人, 的...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>作为推理小说可能不是最好的，作为言情小说可能也不是最好的。但是结合在一起却很让人惊奇。羡慕那...</td>\n",
       "      <td>[作为, 推理小说, 可能, 不是, 最好, 的, ，, 作为, 言情小说, 可能, 也, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>作为女儿6.1的礼物。虽然晚到了几天。等拿到的时候，女儿爱不释手，上洗手间也看，告知不好。竟...</td>\n",
       "      <td>[作为, 女儿, 6.1, 的, 礼物, 。, 虽然, 晚到, 了, 几天, 。, 等, 拿...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>作为妈妈，我个人很喜欢这本书，但是仍在怀疑这本书是否适合给孩子看？这本书以孩子的视角观看我们...</td>\n",
       "      <td>[作为, 妈妈, ，, 我, 个人, 很, 喜欢, 这, 本书, ，, 但是, 仍, 在, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>昨晚看着看着就睡着了，今天早晨醒来就立马抓起继续啃，正逢小说结尾部分，也正如作者的期望，我被...</td>\n",
       "      <td>[昨晚, 看着, 看着, 就, 睡着, 了, ，, 今天, 早晨, 醒来, 就, 立马, 抓...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>昨天我把这套书看完了，结尾我不是很喜欢有点太戏剧了，但对书中的主人公却是最好的安排，我去过新...</td>\n",
       "      <td>[昨天, 我, 把, 这, 套书, 看, 完, 了, ，, 结尾, 我, 不是, 很, 喜欢...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21075</th>\n",
       "      <td>买完降价了，店家服务还不错，但安装售后太黑了安装费竟然漫天要价，一开口要378，我不肯买单，...</td>\n",
       "      <td>[买, 完, 降价, 了, ，, 店家, 服务, 还, 不错, ，, 但, 安装, 售后, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21076</th>\n",
       "      <td>从淘宝客服到送货小哥再到售后检测，态度都很好，都很满意，但是当热水器温度加热到最高温度电源自...</td>\n",
       "      <td>[从, 淘宝, 客服, 到, 送货, 小哥, 再, 到, 售后, 检测, ，, 态度, 都,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21077</th>\n",
       "      <td>帮别人买的，说很好，就是顺丰慢了点，不过还可以了，</td>\n",
       "      <td>[帮别人, 买, 的, ，, 说, 很, 好, ，, 就是, 顺丰慢, 了, 点, ，, 不...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21078</th>\n",
       "      <td>良好信誉的商家从小细节做起，抽奖抽到一半球热水壶上面居然写着 真货&amp;rdquo;！！！！还有...</td>\n",
       "      <td>[良好信誉, 的, 商家, 从小, 细节, 做起, ，, 抽奖, 抽到, 一, 半球, 热,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21079</th>\n",
       "      <td>买完后去实体店看了下50升的才680元，没想到比网购还便宜，买贵了。</td>\n",
       "      <td>[买, 完后, 去, 实体店, 看, 了, 下, 50, 升, 的, 才, 680, 元, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21080</th>\n",
       "      <td>安装费超级贵本来说几十块，安完竟然200块，自己买材料，安装师傅还不给装。卖家倒是服务很好，...</td>\n",
       "      <td>[安装费, 超级, 贵, 本来, 说, 几十块, ，, 安完, 竟然, 200, 块, ，,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21081</th>\n",
       "      <td>还好很美观，安装费太贵花了190元。</td>\n",
       "      <td>[还好, 很, 美观, ，, 安装费, 太贵, 花, 了, 190, 元, 。]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21082</th>\n",
       "      <td>商品包装很好，做工很好！性价比高！但是太原这边Media安装人员收费不便宜，安装用了195元。</td>\n",
       "      <td>[商品, 包装, 很, 好, ，, 做工, 很, 好, ！, 性价比, 高, ！, 但是, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21083</th>\n",
       "      <td>东西不错，现在这个天早上把水烧到最热断电，晚上回去还可以洗澡。就是这款太老了，没有定时器，不...</td>\n",
       "      <td>[东西, 不错, ，, 现在, 这个, 天, 早上, 把, 水烧, 到, 最, 热, 断电,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21084</th>\n",
       "      <td>安全阀那略有问题，不过总体不错，好评吧。</td>\n",
       "      <td>[安全阀, 那, 略有, 问题, ，, 不过, 总体, 不错, ，, 好评, 吧, 。]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21085</th>\n",
       "      <td>东西不错有的速度很快就是安装师傅黑点</td>\n",
       "      <td>[东西, 不错, 有, 的, 速度, 很快, 就是, 安装, 师傅, 黑点]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21086</th>\n",
       "      <td>好，物流也快，就是安装费花了200，有点贵。</td>\n",
       "      <td>[好, ，, 物流, 也, 快, ，, 就是, 安装费, 花, 了, 200, ，, 有点,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21087</th>\n",
       "      <td>东西已经装好了，工人师傅挺好的，就是真的好贵，装完要200，太贵了！</td>\n",
       "      <td>[东西, 已经, 装好, 了, ，, 工人, 师傅, 挺, 好, 的, ，, 就是, 真的,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21088</th>\n",
       "      <td>安装打美的客服电话就可以免费装了，但最好备好材料：3条喉管，1个三通。安装虽然免费，但安装师...</td>\n",
       "      <td>[安装, 打, 美的, 客服, 电话, 就, 可以, 免费, 装, 了, ，, 但, 最好,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21089</th>\n",
       "      <td>东西装修好了，也不知道好不好，可是售后太黑了！买了两个400多的配件费！都无语了！本来在网上...</td>\n",
       "      <td>[东西, 装修, 好, 了, ，, 也, 不, 知道, 好不好, ，, 可是, 售后, 太黑...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21090</th>\n",
       "      <td>昨天刚装好，试用了一下，不错，家里都有热水了，值得购买的东东。就是安装费好贵啊。</td>\n",
       "      <td>[昨天, 刚装, 好, ，, 试用, 了, 一下, ，, 不错, ，, 家里, 都, 有, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21091</th>\n",
       "      <td>加热不是很快，昨天安装上就用了，洗澡洗了半个小时就变成冷水了，不知道是机器的事还是我的事。不...</td>\n",
       "      <td>[加热, 不是, 很快, ，, 昨天, 安装, 上, 就, 用, 了, ，, 洗澡, 洗, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21092</th>\n",
       "      <td>好评 就是安装用的件有点贵 花了160 建议亲们自己买东西让售后安</td>\n",
       "      <td>[好评,  , 就是, 安装, 用, 的, 件, 有点, 贵,  , 花, 了, 160, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21093</th>\n",
       "      <td>很满意 就是安装材料费有点贵花了两百多 棒！ 客服很好 很认真</td>\n",
       "      <td>[很, 满意,  , 就是, 安装, 材料费, 有点, 贵花, 了, 两百多,  , 棒, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21094</th>\n",
       "      <td>还没装上去，美的货信得过哈哈！不过物流真心麻烦，明明是市区，居然要跑到30公里外收货，对物流...</td>\n",
       "      <td>[还, 没, 装上去, ，, 美的, 货, 信得过, 哈哈, ！, 不过, 物流, 真心, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21095</th>\n",
       "      <td>热水器用了一个星期收到！和客服沟通了下，现在就如实评价！！！热水器冷热调节相当不灵敏，太热了...</td>\n",
       "      <td>[热水器, 用, 了, 一个, 星期, 收到, ！, 和, 客服, 沟通, 了, 下, ，,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21096</th>\n",
       "      <td>刚开始说好了，有一个四件套，结果给我忘了发，后来补发8天才到。还说什么浴巾就是两根破毛巾。质...</td>\n",
       "      <td>[刚, 开始, 说好, 了, ，, 有, 一个, 四件套, ，, 结果, 给, 我, 忘, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21097</th>\n",
       "      <td>客服骗人 不是恒温又说是恒温 噪音大 燃气的味道浓 10号开始使用 17号马上没有热水出 差差差差</td>\n",
       "      <td>[客服, 骗人,  , 不是, 恒温, 又, 说, 是, 恒温,  , 噪音, 大,  , ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21098</th>\n",
       "      <td>厨宝还好吧  但那赠品送了也跟没送差不多</td>\n",
       "      <td>[厨宝, 还好, 吧,  ,  , 但, 那, 赠品, 送, 了, 也, 跟, 没, 送, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21099</th>\n",
       "      <td>恰逢春节期间，没有安装工人，所以自己动手两块后安装好了，用不了半小时，效果不错，只是送的赠品...</td>\n",
       "      <td>[恰逢, 春节, 期间, ，, 没有, 安装工人, ，, 所以, 自己, 动手, 两块, 后...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21100</th>\n",
       "      <td>安装我自己花了500多，美的够黑心的，真的是烦心，安装的售后叼的要死！差评！！！！！</td>\n",
       "      <td>[安装, 我, 自己, 花, 了, 500, 多, ，, 美的, 够, 黑心, 的, ，, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21101</th>\n",
       "      <td>东西不错，售后太差，安装一个热水器400块钱，像话吗？钱给完了发现被黑了！</td>\n",
       "      <td>[东西, 不错, ，, 售后, 太, 差, ，, 安装, 一个, 热水器, 400, 块钱,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21102</th>\n",
       "      <td>碰到最差的、最骗人的卖家，好吧，我倒霉！这个卖家太不厚道了，显示所在地上海，我买这个热水器选...</td>\n",
       "      <td>[碰到, 最差, 的, 、, 最, 骗人, 的, 卖家, ，, 好, 吧, ，, 我, 倒霉...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21103</th>\n",
       "      <td>宝贝不错，物流也不错，售后差，</td>\n",
       "      <td>[宝贝, 不错, ，, 物流, 也, 不错, ，, 售后, 差, ，]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21104</th>\n",
       "      <td>美的售后太垃圾，其他售后都是两小时回电话，美的是24小时，结果超过市区不到五公里问我收五十，...</td>\n",
       "      <td>[美的, 售后, 太, 垃圾, ，, 其他, 售后, 都, 是, 两, 小时, 回, 电话,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21105 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       0  \\\n",
       "0      做父母一定要有刘墉这样的心态，不断地学习，不断地进步，不断地给自己补充新鲜血液，让自己保持一...   \n",
       "1      作者真有英国人严谨的风格，提出观点、进行论述论证，尽管本人对物理学了解不深，但是仍然能感受到...   \n",
       "2      作者长篇大论借用详细报告数据处理工作和计算结果支持其新观点。为什么荷兰曾经县有欧洲最高的生产...   \n",
       "3      作者在战几时之前用了＂拥抱＂令人叫绝．日本如果没有战败，就有会有美军的占领，没胡官僚主义的延...   \n",
       "4      作者在少年时即喜阅读，能看出他精读了无数经典，因而他有一个庞大的内心世界。他的作品最难能可贵...   \n",
       "5      作者有一种专业的谨慎，若能有幸学习原版也许会更好，简体版的书中的印刷错误比较多，影响学者理解...   \n",
       "6      作者用诗一样的语言把如水般清澈透明的思想娓娓道来，像一个经验丰富的智慧老人为我们解开一个又一...   \n",
       "7      作者提出了一种工作和生活的方式，作为咨询界的元老，不仅能提出理念，而且能够身体力行地实践，并...   \n",
       "8      作者妙语连珠，将整个60-70年代用层出不穷的摇滚巨星与自身故事紧紧相连什么是乡愁？什么是摇...   \n",
       "9      作者逻辑严密，一气呵成。没有一句废话，深入浅出，循循善诱，环环相扣。让平日里看到指标图释就头...   \n",
       "10     作者力从马克思注意经济学角度来剖析当代中国经济细心的人会发现中国近20年来的一些政策措施在何...   \n",
       "11     作者结合希尔和卡耐基、汪中求等大师的一些观点，结合中国的实际情况，给渴望成功的青年指出一条实...   \n",
       "12     作者更多的是从圆圆母亲的角度来写这个文章。90%的例子都是以圆圆为中心的。我认为，例证不够充...   \n",
       "13     作者对于某些电影“表面粗糙”、“内里光滑”的分析，可谓入木三分，为理解那些貌似触及现实而又让...   \n",
       "14     作者的理念很好，主要是看一个公司的内部情况来决定股票的情况。操作比较难，作者也承认，所以他说...   \n",
       "15     作者的观点独特，语言犀利，深刻的总结了男人与女人之间的是非恩怨，把男人与女人在生活中积累的宿...   \n",
       "16     作者的笔触很真实 将一个不忠却又深爱自己妻子的丈夫形象毫无遮掩的展示给读者 而什么是真爱 什...   \n",
       "17     作者笔下留情啊，深圳的自由作家远远没有《离婚未遂》笔下那么潇洒。事实上，深圳是一个典型的伪文...   \n",
       "18     作者被认为是“爱的奇迹天使”，确实是这样的。很多人认为与孩子的相处一定充满快乐，但却不知一份...   \n",
       "19     作为有史以来最伟大的基金经理彼得&#183;林奇凭借其在投资领域杰出的贡献，终其一生的经验和...   \n",
       "20     作为一名山西太谷人，从小听多了有关晋商的故事，也去过许多山西的地方，但由于离家已久，故乡渐渐...   \n",
       "21     作为一名“白骨精”，我不得不佩服作者入木三分的刻画，不仅精准地描绘出我们的生活画面，更深刻地...   \n",
       "22     作为一本职场启示录，不错没有像杜拉拉一样的引入胜的故事，却也用轻松的方式告诉你，如何在职场中...   \n",
       "23     作为一本屹立不倒的经典教材，纽摄所传授的不是单纯的技术，而是理念。不是教条的灌输，而是互动性...   \n",
       "24     作为一本西方人创作的心理学读物，还是以西方人的视角来看待问题的方式来阐述，目前刚刚开始读，但...   \n",
       "25     作为推理小说可能不是最好的，作为言情小说可能也不是最好的。但是结合在一起却很让人惊奇。羡慕那...   \n",
       "26     作为女儿6.1的礼物。虽然晚到了几天。等拿到的时候，女儿爱不释手，上洗手间也看，告知不好。竟...   \n",
       "27     作为妈妈，我个人很喜欢这本书，但是仍在怀疑这本书是否适合给孩子看？这本书以孩子的视角观看我们...   \n",
       "28     昨晚看着看着就睡着了，今天早晨醒来就立马抓起继续啃，正逢小说结尾部分，也正如作者的期望，我被...   \n",
       "29     昨天我把这套书看完了，结尾我不是很喜欢有点太戏剧了，但对书中的主人公却是最好的安排，我去过新...   \n",
       "...                                                  ...   \n",
       "21075  买完降价了，店家服务还不错，但安装售后太黑了安装费竟然漫天要价，一开口要378，我不肯买单，...   \n",
       "21076  从淘宝客服到送货小哥再到售后检测，态度都很好，都很满意，但是当热水器温度加热到最高温度电源自...   \n",
       "21077                          帮别人买的，说很好，就是顺丰慢了点，不过还可以了，   \n",
       "21078  良好信誉的商家从小细节做起，抽奖抽到一半球热水壶上面居然写着 真货&rdquo;！！！！还有...   \n",
       "21079                 买完后去实体店看了下50升的才680元，没想到比网购还便宜，买贵了。   \n",
       "21080  安装费超级贵本来说几十块，安完竟然200块，自己买材料，安装师傅还不给装。卖家倒是服务很好，...   \n",
       "21081                                 还好很美观，安装费太贵花了190元。   \n",
       "21082    商品包装很好，做工很好！性价比高！但是太原这边Media安装人员收费不便宜，安装用了195元。   \n",
       "21083  东西不错，现在这个天早上把水烧到最热断电，晚上回去还可以洗澡。就是这款太老了，没有定时器，不...   \n",
       "21084                               安全阀那略有问题，不过总体不错，好评吧。   \n",
       "21085                                 东西不错有的速度很快就是安装师傅黑点   \n",
       "21086                             好，物流也快，就是安装费花了200，有点贵。   \n",
       "21087                 东西已经装好了，工人师傅挺好的，就是真的好贵，装完要200，太贵了！   \n",
       "21088  安装打美的客服电话就可以免费装了，但最好备好材料：3条喉管，1个三通。安装虽然免费，但安装师...   \n",
       "21089  东西装修好了，也不知道好不好，可是售后太黑了！买了两个400多的配件费！都无语了！本来在网上...   \n",
       "21090           昨天刚装好，试用了一下，不错，家里都有热水了，值得购买的东东。就是安装费好贵啊。   \n",
       "21091  加热不是很快，昨天安装上就用了，洗澡洗了半个小时就变成冷水了，不知道是机器的事还是我的事。不...   \n",
       "21092                  好评 就是安装用的件有点贵 花了160 建议亲们自己买东西让售后安   \n",
       "21093                    很满意 就是安装材料费有点贵花了两百多 棒！ 客服很好 很认真   \n",
       "21094  还没装上去，美的货信得过哈哈！不过物流真心麻烦，明明是市区，居然要跑到30公里外收货，对物流...   \n",
       "21095  热水器用了一个星期收到！和客服沟通了下，现在就如实评价！！！热水器冷热调节相当不灵敏，太热了...   \n",
       "21096  刚开始说好了，有一个四件套，结果给我忘了发，后来补发8天才到。还说什么浴巾就是两根破毛巾。质...   \n",
       "21097  客服骗人 不是恒温又说是恒温 噪音大 燃气的味道浓 10号开始使用 17号马上没有热水出 差差差差   \n",
       "21098                               厨宝还好吧  但那赠品送了也跟没送差不多   \n",
       "21099  恰逢春节期间，没有安装工人，所以自己动手两块后安装好了，用不了半小时，效果不错，只是送的赠品...   \n",
       "21100         安装我自己花了500多，美的够黑心的，真的是烦心，安装的售后叼的要死！差评！！！！！   \n",
       "21101              东西不错，售后太差，安装一个热水器400块钱，像话吗？钱给完了发现被黑了！   \n",
       "21102  碰到最差的、最骗人的卖家，好吧，我倒霉！这个卖家太不厚道了，显示所在地上海，我买这个热水器选...   \n",
       "21103                                    宝贝不错，物流也不错，售后差，   \n",
       "21104  美的售后太垃圾，其他售后都是两小时回电话，美的是24小时，结果超过市区不到五公里问我收五十，...   \n",
       "\n",
       "                                                   words  \n",
       "0      [做, 父母, 一定, 要, 有, 刘墉, 这样, 的, 心态, ，, 不断, 地, 学习,...  \n",
       "1      [作者, 真有, 英国人, 严谨, 的, 风格, ，, 提出, 观点, 、, 进行, 论述,...  \n",
       "2      [作者, 长篇大论, 借用, 详细, 报告, 数据处理, 工作, 和, 计算结果, 支持, ...  \n",
       "3      [作者, 在, 战, 几时, 之前, 用, 了, ＂, 拥抱, ＂, 令人, 叫绝, ．, ...  \n",
       "4      [作者, 在, 少年, 时即, 喜, 阅读, ，, 能, 看出, 他, 精读, 了, 无数,...  \n",
       "5      [作者, 有, 一种, 专业, 的, 谨慎, ，, 若能, 有幸, 学习, 原版, 也许, ...  \n",
       "6      [作者, 用, 诗, 一样, 的, 语言, 把, 如水般, 清澈, 透明, 的, 思想, 娓...  \n",
       "7      [作者, 提出, 了, 一种, 工作, 和, 生活, 的, 方式, ，, 作为, 咨询, 界...  \n",
       "8      [作者, 妙语连珠, ，, 将, 整个, 60, -, 70, 年代, 用, 层出不穷, 的...  \n",
       "9      [作者, 逻辑, 严密, ，, 一气呵成, 。, 没有, 一句, 废话, ，, 深入浅出, ...  \n",
       "10     [作者, 力, 从, 马克思, 注意, 经济学, 角度, 来, 剖析, 当代, 中国, 经济...  \n",
       "11     [作者, 结合, 希尔, 和, 卡耐基, 、, 汪中求, 等, 大师, 的, 一些, 观点,...  \n",
       "12     [作者, 更, 多, 的, 是从, 圆圆, 母亲, 的, 角度, 来, 写, 这个, 文章,...  \n",
       "13     [作者, 对于, 某些, 电影, “, 表面, 粗糙, ”, 、, “, 内里, 光滑, ”...  \n",
       "14     [作者, 的, 理念, 很, 好, ，, 主要, 是, 看, 一个, 公司, 的, 内部, ...  \n",
       "15     [作者, 的, 观点, 独特, ，, 语言, 犀利, ，, 深刻, 的, 总结, 了, 男人...  \n",
       "16     [作者, 的, 笔触, 很, 真实,  , 将, 一个, 不忠, 却, 又, 深爱, 自己,...  \n",
       "17     [作者, 笔下留情, 啊, ，, 深圳, 的, 自由, 作家, 远远, 没有, 《, 离婚,...  \n",
       "18     [作者, 被, 认为, 是, “, 爱, 的, 奇迹, 天使, ”, ，, 确实, 是, 这...  \n",
       "19     [作为, 有史以来, 最, 伟大, 的, 基金, 经理, 彼得, &#, 183, ;, 林...  \n",
       "20     [作为, 一名, 山西, 太谷, 人, ，, 从小, 听, 多, 了, 有关, 晋商, 的,...  \n",
       "21     [作为, 一名, “, 白骨精, ”, ，, 我, 不得不, 佩服, 作者, 入木三分, 的...  \n",
       "22     [作为, 一本, 职场, 启示录, ，, 不错, 没有, 像, 杜, 拉拉, 一样, 的, ...  \n",
       "23     [作为, 一本, 屹立, 不倒, 的, 经典, 教材, ，, 纽摄, 所, 传授, 的, 不...  \n",
       "24     [作为, 一本, 西方人, 创作, 的, 心理学, 读物, ，, 还是, 以, 西方人, 的...  \n",
       "25     [作为, 推理小说, 可能, 不是, 最好, 的, ，, 作为, 言情小说, 可能, 也, ...  \n",
       "26     [作为, 女儿, 6.1, 的, 礼物, 。, 虽然, 晚到, 了, 几天, 。, 等, 拿...  \n",
       "27     [作为, 妈妈, ，, 我, 个人, 很, 喜欢, 这, 本书, ，, 但是, 仍, 在, ...  \n",
       "28     [昨晚, 看着, 看着, 就, 睡着, 了, ，, 今天, 早晨, 醒来, 就, 立马, 抓...  \n",
       "29     [昨天, 我, 把, 这, 套书, 看, 完, 了, ，, 结尾, 我, 不是, 很, 喜欢...  \n",
       "...                                                  ...  \n",
       "21075  [买, 完, 降价, 了, ，, 店家, 服务, 还, 不错, ，, 但, 安装, 售后, ...  \n",
       "21076  [从, 淘宝, 客服, 到, 送货, 小哥, 再, 到, 售后, 检测, ，, 态度, 都,...  \n",
       "21077  [帮别人, 买, 的, ，, 说, 很, 好, ，, 就是, 顺丰慢, 了, 点, ，, 不...  \n",
       "21078  [良好信誉, 的, 商家, 从小, 细节, 做起, ，, 抽奖, 抽到, 一, 半球, 热,...  \n",
       "21079  [买, 完后, 去, 实体店, 看, 了, 下, 50, 升, 的, 才, 680, 元, ...  \n",
       "21080  [安装费, 超级, 贵, 本来, 说, 几十块, ，, 安完, 竟然, 200, 块, ，,...  \n",
       "21081           [还好, 很, 美观, ，, 安装费, 太贵, 花, 了, 190, 元, 。]  \n",
       "21082  [商品, 包装, 很, 好, ，, 做工, 很, 好, ！, 性价比, 高, ！, 但是, ...  \n",
       "21083  [东西, 不错, ，, 现在, 这个, 天, 早上, 把, 水烧, 到, 最, 热, 断电,...  \n",
       "21084       [安全阀, 那, 略有, 问题, ，, 不过, 总体, 不错, ，, 好评, 吧, 。]  \n",
       "21085             [东西, 不错, 有, 的, 速度, 很快, 就是, 安装, 师傅, 黑点]  \n",
       "21086  [好, ，, 物流, 也, 快, ，, 就是, 安装费, 花, 了, 200, ，, 有点,...  \n",
       "21087  [东西, 已经, 装好, 了, ，, 工人, 师傅, 挺, 好, 的, ，, 就是, 真的,...  \n",
       "21088  [安装, 打, 美的, 客服, 电话, 就, 可以, 免费, 装, 了, ，, 但, 最好,...  \n",
       "21089  [东西, 装修, 好, 了, ，, 也, 不, 知道, 好不好, ，, 可是, 售后, 太黑...  \n",
       "21090  [昨天, 刚装, 好, ，, 试用, 了, 一下, ，, 不错, ，, 家里, 都, 有, ...  \n",
       "21091  [加热, 不是, 很快, ，, 昨天, 安装, 上, 就, 用, 了, ，, 洗澡, 洗, ...  \n",
       "21092  [好评,  , 就是, 安装, 用, 的, 件, 有点, 贵,  , 花, 了, 160, ...  \n",
       "21093  [很, 满意,  , 就是, 安装, 材料费, 有点, 贵花, 了, 两百多,  , 棒, ...  \n",
       "21094  [还, 没, 装上去, ，, 美的, 货, 信得过, 哈哈, ！, 不过, 物流, 真心, ...  \n",
       "21095  [热水器, 用, 了, 一个, 星期, 收到, ！, 和, 客服, 沟通, 了, 下, ，,...  \n",
       "21096  [刚, 开始, 说好, 了, ，, 有, 一个, 四件套, ，, 结果, 给, 我, 忘, ...  \n",
       "21097  [客服, 骗人,  , 不是, 恒温, 又, 说, 是, 恒温,  , 噪音, 大,  , ...  \n",
       "21098  [厨宝, 还好, 吧,  ,  , 但, 那, 赠品, 送, 了, 也, 跟, 没, 送, ...  \n",
       "21099  [恰逢, 春节, 期间, ，, 没有, 安装工人, ，, 所以, 自己, 动手, 两块, 后...  \n",
       "21100  [安装, 我, 自己, 花, 了, 500, 多, ，, 美的, 够, 黑心, 的, ，, ...  \n",
       "21101  [东西, 不错, ，, 售后, 太, 差, ，, 安装, 一个, 热水器, 400, 块钱,...  \n",
       "21102  [碰到, 最差, 的, 、, 最, 骗人, 的, 卖家, ，, 好, 吧, ，, 我, 倒霉...  \n",
       "21103                [宝贝, 不错, ，, 物流, 也, 不错, ，, 售后, 差, ，]  \n",
       "21104  [美的, 售后, 太, 垃圾, ，, 其他, 售后, 都, 是, 两, 小时, 回, 电话,...  \n",
       "\n",
       "[21105 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1804"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 一行数据最多的词汇数\n",
    "max_document_length = max([len(x) for x in pn['words']])\n",
    "max_document_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置一个评论最多1000个词\n",
    "max_document_length = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [' '.join(x) for x in pn['words']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'宝贝 不错 ， 物流 也 不错 ， 售后 差 ，'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看一条评论\n",
    "texts[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实例化分词器，设置字典中最大词汇数为30000\n",
    "tokenizer = Tokenizer(num_words=30000)\n",
    "# 传入我们的训练数据，建立词典\n",
    "tokenizer.fit_on_texts(texts) \n",
    "# 把词转换为编号，词的编号根据词频设定，频率越大，编号越小\n",
    "sequences = tokenizer.texts_to_sequences(texts) \n",
    "# 把序列设定为1000的长度，超过1000的部分舍弃，不到1000则补0\n",
    "sequences = pad_sequences(sequences, maxlen=1000, padding='post')  \n",
    "sequences = np.array(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 词对应编号的字典\n",
    "dict_text = tokenizer.word_index\n",
    "dict_text['也']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1318,   24,    1, 1482,    9,   24,    1,  909,  156,    1,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义标签\n",
    "positive_labels = [[0, 1] for _ in range(poslen)]\n",
    "negative_labels = [[1, 0] for _ in range(neglen)]\n",
    "y = np.concatenate([positive_labels, negative_labels], 0)\n",
    "\n",
    "# 打乱数据\n",
    "np.random.seed(10)\n",
    "shuffle_indices = np.random.permutation(np.arange(len(y)))\n",
    "x_shuffled = sequences[shuffle_indices]\n",
    "y_shuffled = y[shuffle_indices]\n",
    "\n",
    "# 数据集切分为两部分\n",
    "test_sample_index = -1 * int(0.1 * float(len(y)))\n",
    "x_train, x_test = x_shuffled[:test_sample_index], x_shuffled[test_sample_index:]\n",
    "y_train, y_test = y_shuffled[:test_sample_index], y_shuffled[test_sample_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 1000)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 1000, 128)    3840000     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 998, 32)      12320       embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 997, 32)      16416       embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)              (None, 996, 32)      20512       embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling1D) (None, 199, 32)      0           conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling1D) (None, 199, 32)      0           conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling1D) (None, 199, 32)      0           conv1d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 197, 32)      3104        max_pooling1d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, 196, 32)      4128        max_pooling1d_13[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_17 (Conv1D)              (None, 195, 32)      5152        max_pooling1d_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling1D) (None, 39, 32)       0           conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling1D) (None, 39, 32)       0           conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling1D) (None, 39, 32)       0           conv1d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 37, 32)       3104        max_pooling1d_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, 36, 32)       4128        max_pooling1d_14[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_18 (Conv1D)              (None, 35, 32)       5152        max_pooling1d_17[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling1D) (None, 1, 32)        0           conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling1D) (None, 1, 32)        0           conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling1D) (None, 1, 32)        0           conv1d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 32)           0           max_pooling1d_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, 32)           0           max_pooling1d_15[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_6 (Flatten)             (None, 32)           0           max_pooling1d_18[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 96)           0           flatten_4[0][0]                  \n",
      "                                                                 flatten_5[0][0]                  \n",
      "                                                                 flatten_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 128)          12416       concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 128)          0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 2)            258         dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 3,926,690\n",
      "Trainable params: 3,926,690\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 定义函数式模型\n",
    "# 模型输入\n",
    "sequence_input = Input(shape=(1000,))\n",
    "# Embedding层，30000表示30000个词，每个词对应的向量为128维，序列长度为1000\n",
    "embedding_layer = Embedding(30000,\n",
    "                            128,\n",
    "                            input_length=1000)\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "# 卷积核大小为3\n",
    "cnn1 = Conv1D(filters=32, kernel_size=3, activation='relu')(embedded_sequences)\n",
    "cnn1 = MaxPooling1D(pool_size=5)(cnn1)\n",
    "cnn1 = Conv1D(filters=32, kernel_size=3, activation='relu')(cnn1)\n",
    "cnn1 = MaxPooling1D(pool_size=5)(cnn1)\n",
    "cnn1 = Conv1D(filters=32, kernel_size=3, activation='relu')(cnn1)\n",
    "cnn1 = MaxPooling1D(pool_size=37)(cnn1)\n",
    "cnn1 = Flatten()(cnn1)\n",
    "# 卷积核大小为4\n",
    "cnn2 = Conv1D(filters=32, kernel_size=4, activation='relu')(embedded_sequences)\n",
    "cnn2 = MaxPooling1D(pool_size=5)(cnn2)\n",
    "cnn2 = Conv1D(filters=32, kernel_size=4, activation='relu')(cnn2)\n",
    "cnn2 = MaxPooling1D(pool_size=5)(cnn2)\n",
    "cnn2 = Conv1D(filters=32, kernel_size=4, activation='relu')(cnn2)\n",
    "cnn2 = MaxPooling1D(pool_size=36)(cnn2)\n",
    "cnn2 = Flatten()(cnn2)\n",
    "# 卷积核大小为5\n",
    "cnn3 = Conv1D(filters=32, kernel_size=5, activation='relu')(embedded_sequences)\n",
    "cnn3 = MaxPooling1D(pool_size=5)(cnn3)\n",
    "cnn3 = Conv1D(filters=32, kernel_size=5, activation='relu')(cnn3)\n",
    "cnn3 = MaxPooling1D(pool_size=5)(cnn3)\n",
    "cnn3 = Conv1D(filters=32, kernel_size=5, activation='relu')(cnn3)\n",
    "cnn3 = MaxPooling1D(pool_size=35)(cnn3)\n",
    "cnn3 = Flatten()(cnn3)\n",
    "\n",
    "# 合并\n",
    "merge = concatenate([cnn1, cnn2, cnn3], axis=1)\n",
    "# 全链接层\n",
    "x = Dense(128, activation='relu')(merge)\n",
    "# Dropout层\n",
    "x = Dropout(0.5)(x)\n",
    "# 输出层\n",
    "preds = Dense(2, activation='softmax')(x)\n",
    "# 定义模型\n",
    "model = Model(sequence_input, preds)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18995 samples, validate on 2110 samples\n",
      "Epoch 1/5\n",
      "18995/18995 [==============================] - 48s 3ms/step - loss: 0.4896 - acc: 0.7411 - val_loss: 0.2271 - val_acc: 0.9180\n",
      "Epoch 2/5\n",
      "18995/18995 [==============================] - 31s 2ms/step - loss: 0.1497 - acc: 0.9474 - val_loss: 0.1700 - val_acc: 0.9351\n",
      "Epoch 3/5\n",
      "18995/18995 [==============================] - 31s 2ms/step - loss: 0.0526 - acc: 0.9849 - val_loss: 0.2323 - val_acc: 0.9313\n",
      "Epoch 4/5\n",
      "18995/18995 [==============================] - 31s 2ms/step - loss: 0.0210 - acc: 0.9955 - val_loss: 0.2392 - val_acc: 0.9294\n",
      "Epoch 5/5\n",
      "18995/18995 [==============================] - 31s 2ms/step - loss: 0.0114 - acc: 0.9977 - val_loss: 0.2639 - val_acc: 0.9313\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x29c7f2a9588>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 训练模型\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['acc'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=128,\n",
    "          epochs=5,\n",
    "          validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 预测\n",
    "def predict(text):\n",
    "    # 对句子分词\n",
    "    cw = list(jieba.cut(text)) \n",
    "    word_id = []\n",
    "    # 把词转换为编号\n",
    "    for word in cw:\n",
    "        try:\n",
    "            temp = dict_text[word]\n",
    "            word_id.append(temp)\n",
    "        except:\n",
    "            word_id.append(0)\n",
    "    word_id = np.array(word_id)\n",
    "    word_id = word_id[np.newaxis,:]\n",
    "    sequences = pad_sequences(word_id, maxlen=1000, padding='post')  \n",
    "    result = np.argmax(model.predict(sequences))\n",
    "    if(result==1):\n",
    "        print(\"positive comment\")\n",
    "    else:\n",
    "        print(\"negative comment\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive comment\n"
     ]
    }
   ],
   "source": [
    "predict(\"东西质量不错，下次还会再来买\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
